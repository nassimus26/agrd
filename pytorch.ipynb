{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "gvdYoTqq5hQl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148611950,
     "user_tz": -60,
     "elapsed": 724,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:05.814325Z",
     "start_time": "2024-10-29T04:52:05.801774Z"
    }
   },
   "source": [
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2079,
     "status": "ok",
     "timestamp": 1730148614026,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     },
     "user_tz": -60
    },
    "id": "hRi-J2JoUIS-",
    "outputId": "4a7ca52a-be31-422f-f609-2625fa925a86",
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:05.899595Z",
     "start_time": "2024-10-29T04:52:05.883971Z"
    }
   },
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y5tJLHNhUXHg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148614026,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:06.074235Z",
     "start_time": "2024-10-29T04:52:06.067628Z"
    }
   },
   "source": [
    "if RunningInCOLAB:\n",
    "  my_file = Path(\"trainm\")\n",
    "  if my_file.exists()==False:\n",
    "     LOADED = False\n",
    "     !unzip drive/\"My Drive\"/trainm.zip"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nOBtj0HT09pG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148614026,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:06.300860Z",
     "start_time": "2024-10-29T04:52:06.099899Z"
    }
   },
   "source": [
    "if False and RunningInCOLAB:\n",
    "  kaggle_folder = Path(\".kaggle\")\n",
    "  if Path(\"rvdata\").exists()==False:\n",
    "    !pip install -q kaggle\n",
    "    !mkdir ~/.kaggle\n",
    "    !cp drive/\"My Drive\"/kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    !kaggle datasets download -d mohamedmustafa/real-life-violence-situations-dataset\n",
    "    !unzip real-life-violence-situations-dataset.zip\n",
    "    !rm real-life-violence-situations-dataset.zip\n",
    "    !rm -R \"real life violence situations\"\n",
    "    !mv -T \"Real Life Violence Dataset\" \"rvdata\"\n",
    "    !kaggle datasets download -d vulamnguyen/rwf2000\n",
    "    !unzip rwf2000.zip\n",
    "    !rm rwf2000.zip\n",
    "    !mv RWF-2000/train/* RWF-2000/\n",
    "    !mv RWF-2000/val/Fight/* RWF-2000/Fight\n",
    "    !mv RWF-2000/val/NonFight/* RWF-2000/NonFight\n",
    "    !rm -R RWF-2000/train/\n",
    "    !rm -R RWF-2000/val/\n",
    "    #!git clone https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos.git\n",
    "    #!mv -T A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset vdd\n",
    "    #!rm -R A-Dataset-for-Automatic-Violence-Detection-in-Videos\n",
    "    !kaggle datasets download -d toluwaniaremu/smartcity-cctv-violence-detection-dataset-scvd\n",
    "    !unzip smartcity-cctv-violence-detection-dataset-scvd.zip\n",
    "    !rm smartcity-cctv-violence-detection-dataset-scvd.zip\n",
    "    !rm -R SCVD/SCVD_converted_sec_split\n",
    "    !mkdir SCVD/NonViolence\n",
    "    !mkdir SCVD/Violence\n",
    "    !mv SCVD/SCVD_converted/Train/Normal/* SCVD/NonViolence\n",
    "    !mv SCVD/SCVD_converted/Train/Violence/* SCVD/Violence\n",
    "    !mv SCVD/SCVD_converted/Train/Weaponized/* SCVD/Violence\n",
    "    !mv SCVD/SCVD_converted/Test/Normal/* SCVD/NonViolence\n",
    "    !mv SCVD/SCVD_converted/Test/Violence/* SCVD/Violence\n",
    "    !mv SCVD/SCVD_converted/Test/Weaponized/* SCVD/Violence\n",
    "    !rm -R SCVD/SCVD_converted"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YWSGwMQ3cbJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148614026,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "outputId": "2591c336-1144-4e45-ddc4-da4d7fbaf983",
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:06.352843Z",
     "start_time": "2024-10-29T04:52:06.328963Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "mono = False\n",
    "dtype=np.uint8\n",
    "max_video_size = 10*1024*1024\n",
    "channel_first = True\n",
    "num_classes = 2\n",
    "frame_readed = 500\n",
    "frame_pages = 20\n",
    "nbr_frame = 6\n",
    "img_width = 320\n",
    "img_height = 320\n",
    "img_size = (img_height, img_width)\n",
    "if channel_first:\n",
    "  input_shape = (3,) + img_size\n",
    "else:\n",
    "  input_shape = img_size + (3,)\n",
    "full_input_shape = (nbr_frame,) + input_shape\n",
    "print(full_input_shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 320, 320)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EdqPe6Uat9Dc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148614026,
     "user_tz": -60,
     "elapsed": 4,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:13.350125Z",
     "start_time": "2024-10-29T04:52:06.595243Z"
    }
   },
   "source": [
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac/miniconda3/envs/ai/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/mac/miniconda3/envs/ai/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/mac/miniconda3/envs/ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4vuhsEp2zS1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148616176,
     "user_tz": -60,
     "elapsed": 2154,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "outputId": "a62ae34b-71a1-40fe-90ad-5fe046dc6a29",
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:13.366350Z",
     "start_time": "2024-10-29T04:52:13.359847Z"
    }
   },
   "source": [
    "if RunningInCOLAB:\n",
    "    !pip install torchinfo"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zGvCWoYct9Dd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148616177,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:17.350359Z",
     "start_time": "2024-10-29T04:52:13.417138Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "tutils = tf.keras.utils"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 05:52:14.071619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730177534.119686   29246 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730177534.134094   29246 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0EcAbmkDt9De",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148616177,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:24.369213Z",
     "start_time": "2024-10-29T04:52:17.366608Z"
    }
   },
   "source": [
    "import os\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision as tv\n",
    "from torchvision import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Normalize,\n",
    "    RandomCrop,\n",
    ")\n",
    "from torchvision.transforms import Compose\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "from torch.utils.data import DataLoader\n",
    "plt.ion()\n",
    "\n",
    "import torchvision.models as models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import tensorflow.keras.utils as tf_utils"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0KJ98zybT6_s",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148616177,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:24.897757Z",
     "start_time": "2024-10-29T04:52:24.407293Z"
    }
   },
   "source": [
    "import cv2\n",
    "def capture(filename):\n",
    "  frames = np.zeros( (frame_readed,) +input_shape, dtype=dtype)\n",
    "  i = 0\n",
    "  vc = cv2.VideoCapture(filename)\n",
    "  while i<frame_readed:\n",
    "      if vc.isOpened():\n",
    "          rval, frame = vc.read()\n",
    "      else:\n",
    "          print( 'Failed to read the video ' + filename)\n",
    "          return frames\n",
    "      if rval==False:\n",
    "          break\n",
    "      frm = cv2.resize(frame, (img_width, img_height))\n",
    "      #frm = tf.keras.applications.mobilenet.preprocess_input(frm)\n",
    "      frm = np.expand_dims(frm, axis=0)\n",
    "      #if np.max(frm)>1:\n",
    "      #  frm = frm/255.0\n",
    "      if channel_first:\n",
    "        frm = frm.transpose(0,3,2,1)\n",
    "      frames[i][:] = frm\n",
    "      i +=1\n",
    "      if i==(nbr_frame*30):\n",
    "          break\n",
    "  vc.release()\n",
    "  step = int(i/nbr_frame)\n",
    "  if (step>8):\n",
    "    step=8\n",
    "  start = int((i - step*nbr_frame)/2)\n",
    "  resf = np.empty( (nbr_frame,) + input_shape )\n",
    "  k = 0\n",
    "  while k<nbr_frame:\n",
    "      j = int(step*k)\n",
    "      resf[k][:] = frames[start+j]\n",
    "      k = k+1\n",
    "  return resf\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x_DHfpuv4eje",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148616177,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:24.928848Z",
     "start_time": "2024-10-29T04:52:24.909544Z"
    }
   },
   "source": [
    "def loadData(dataset, ist, ito, fts_train, labs_train):\n",
    "  print('Loading ' + dataset +' from ' + str(ist) + ' to ' + str(ito) )\n",
    "  page_size = ito-ist\n",
    "  i= 0\n",
    "  for x in os.listdir('./' + dataset):\n",
    "    lx = x.lower()\n",
    "    violence = 1 if (lx.startswith('fight') or lx.startswith('viol')) else 0\n",
    "    td = dataset+'/'+x+'/'\n",
    "    notGood = 'fight' not in x.lower() and 'viol' not in x.lower()\n",
    "    if notGood or os.path.isdir(td)==False:\n",
    "      continue\n",
    "    n= 0\n",
    "    #print('-> folder ' +td)\n",
    "    nbrFiles = 0\n",
    "    for file in Path(td).rglob(\"*\"):\n",
    "      fl = os.path.join('', file)\n",
    "      #print('file ' + fl)\n",
    "      #print(os.path.getsize(fl))\n",
    "      if file.is_dir():\n",
    "        continue\n",
    "      if os.path.getsize(fl)>max_video_size:\n",
    "        continue\n",
    "      nbrFiles += 1\n",
    "    #print('nbrFiles '+str(nbrFiles))\n",
    "    if ito > nbrFiles:\n",
    "      ito = nbrFiles\n",
    "      ist = nbrFiles-page_size\n",
    "      print( 'ist - ito ' + str(ist) + ' - ' + str(ito) )\n",
    "    for file in Path(td).rglob(\"*\"):\n",
    "      fl = os.path.join('', file)\n",
    "      if file.is_dir():\n",
    "        continue\n",
    "      if os.path.getsize(fl)>max_video_size:\n",
    "        continue\n",
    "      #print('visiting ' + fl)\n",
    "      if n<ist:\n",
    "        n+=1\n",
    "        continue\n",
    "      if n>=ito:\n",
    "        break\n",
    "      videos = capture(fl)\n",
    "      fts_train[i][:][:] = videos\n",
    "      labs_train[i] = violence\n",
    "      i+=1\n",
    "      n+=1\n",
    "    ###### this part is not usefull anymore\n",
    "    print(\"Readed \"+str(i))\n",
    "    half= int(len(fts_train)/2)\n",
    "    if i<half:\n",
    "      while i<half:\n",
    "        print(\"Random repeat \"+str(i)+'/'+str(half))\n",
    "        p = np.random.randint(0, i-1)\n",
    "        fts_train[i][:][:] = fts_train[p]\n",
    "        labs_train[i] = labs_train[p]\n",
    "        i+=1\n",
    "    else:\n",
    "      if i>half:\n",
    "        while i<len(fts_train):\n",
    "          print(\"Random repeat \"+str(i)+'/'+str(half))\n",
    "          p = np.random.randint(half+1, len(fts_train)-1)\n",
    "          fts_train[i][:][:] = fts_train[p]\n",
    "          labs_train[i] = labs_train[p]\n",
    "          i+=1\n",
    "  #np.save('./data.npy', fts_train)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M69QuHla05MK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148622081,
     "user_tz": -60,
     "elapsed": 5910,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "outputId": "1d85b612-88fc-445e-b740-ee091700e0cd",
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:25.050969Z",
     "start_time": "2024-10-29T04:52:24.974341Z"
    }
   },
   "source": [
    "if RunningInCOLAB:\n",
    "    !pip install onnx\n",
    "    !pip install onnxscript"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NeEOfIbXIP6O",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148622082,
     "user_tz": -60,
     "elapsed": 7,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:25.150713Z",
     "start_time": "2024-10-29T04:52:25.104464Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import torch.nn.functional as F\n",
    "\n",
    "debugMode = False\n",
    "def toCategorical(tensor, classes):\n",
    "    return F.one_hot(tensor, num_classes=classes)\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X1, X2, X3, X4, X5, X6, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X1, X2, X3, X4, X5, X6, y = X1.to(device), X2.to(device),X3.to(device),X4.to(device),X5.to(device),X6.to(device),y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X1, X2, X3, X4, X5, X6)\n",
    "        if debugMode:\n",
    "          print(\"y_pred\")\n",
    "          print(y_pred)\n",
    "          print(\"y\")\n",
    "          print(y)\n",
    "        # 2. Calculate and accumulate loss\n",
    "        loss = loss_fn( y_pred, y )\n",
    "        train_loss += loss.item()\n",
    "        if debugMode:\n",
    "          print(\"loss\")\n",
    "          print(loss)\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        softmax = torch.softmax(y_pred, dim=0)\n",
    "\n",
    "        if debugMode:\n",
    "          print(\"softmax\")\n",
    "          print(softmax)\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(softmax, dim=1)\n",
    "        y_pred_class = toCategorical(y_pred_class, num_classes)\n",
    "        if debugMode:\n",
    "          print(\"y_pred_class\")\n",
    "          print(y_pred_class)\n",
    "        lgn = (len(y_pred)*num_classes)\n",
    "        if debugMode :\n",
    "          print(\"lgn\")\n",
    "          print(lgn)\n",
    "        train_acc += (y_pred_class == y).sum().item()/lgn\n",
    "        if debugMode:\n",
    "          print(\"y_pred_class\")\n",
    "          print(y_pred_class)\n",
    "          print(\"(y_pred_class == y).sum()\")\n",
    "          print((y_pred_class == y).sum())\n",
    "          print(\"(y_pred_class == y).sum().item\")\n",
    "          print((y_pred_class == y).sum().item())\n",
    "          print(\"train_acc\")\n",
    "          print(train_acc)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X1, X2, X3, X4, X5, X6, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X1, X2, X3, X4, X5, X6, y = X1.to(device), X2.to(device),X3.to(device),X4.to(device),X5.to(device),X6.to(device),y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X1, X2, X3, X4, X5, X6)\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            if debugMode:\n",
    "              print(\"test_pred_labels\")\n",
    "              print(test_pred_labels)\n",
    "            test_pred_labels = toCategorical(test_pred_labels, num_classes)\n",
    "            if debugMode:\n",
    "              print(\"test_pred_labels\")\n",
    "              print(test_pred_labels)\n",
    "            lgn = len(test_pred_logits)*num_classes\n",
    "            if debugMode:\n",
    "              print(\"lgn\")\n",
    "              print(lgn)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/lgn)\n",
    "            if debugMode:\n",
    "              print(\"test_acc\")\n",
    "              print(test_acc)\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def multiInputTrain(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Make sure model on target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "georvHVDfqmE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730148622082,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-29T04:52:26.342956Z",
     "start_time": "2024-10-29T04:52:25.196460Z"
    }
   },
   "source": [
    "all_models = models.list_models()\n",
    "classification_models = models.list_models(module=models)\n",
    "# Initialize models\n",
    "weights = tv.models.EfficientNet_B3_Weights.DEFAULT # .DEFAULT = best available weights\n",
    "b_size = 20\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "class MultiInputs(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MultiInputs, self).__init__()\n",
    "    cnn = tv.models.efficientnet_b3(weights=weights)\n",
    "    for param in cnn.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    cnn.classifier[1] = nn.Flatten(1)\n",
    "    self.cnn = cnn\n",
    "    self.sec = torch.nn.Sequential()\n",
    "    self.sec.append(torch.nn.Flatten(1)),\n",
    "    self.sec.append(torch.nn.Linear(9216, 120))\n",
    "    self.sec.append(torch.nn.ReLU())\n",
    "    self.sec.append(torch.nn.Dropout(p=0.1))\n",
    "    self.sec.append(torch.nn.Linear(120, 24))\n",
    "    self.sec.append(torch.nn.Sigmoid())\n",
    "    self.sec.append(torch.nn.Dropout(p=0.1))\n",
    "    self.sec.append(torch.nn.Linear(24, num_classes, bias=True))\n",
    "  def forward(self, input1, input2, input3, input4, input5, input6):\n",
    "    c1 = self.cnn(input1).view(-1, 1536)\n",
    "    c2 = self.cnn(input2).view(-1, 1536)\n",
    "    c3 = self.cnn(input3).view(-1, 1536)\n",
    "    c4 = self.cnn(input4).view(-1, 1536)\n",
    "    c5 = self.cnn(input5).view(-1, 1536)\n",
    "    c6 = self.cnn(input6).view(-1, 1536)\n",
    "    all = torch.cat((c1, c2, c3, c4, c5, c6), 1)\n",
    "    #all = all.view(1, 1, 9216)\n",
    "    return self.sec(all)\n",
    "model = MultiInputs().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "# Set the random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Start the timer\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, labels) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        side_size = 310\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        crop_size = 300\n",
    "        self.transform = Compose([\n",
    "                    CenterCrop(crop_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    Normalize(mean, std),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def getItem(self, idx):\n",
    "        return self.__getitem__(idx)\n",
    "    def __getitem__(self, idx):\n",
    "        pixels = self.dataset[idx]\n",
    "        frames=[0, 1, 2, 3, 4, 5];\n",
    "        for i in range(nbr_frame):\n",
    "            frm = pixels[i]\n",
    "            if channel_first:\n",
    "              frm = frm.transpose(2,1,0)\n",
    "            pil_img = Image.fromarray(np.uint8(frm)).convert('RGB')\n",
    "            video_data = self.transform(pil_img)\n",
    "            frames[i] = video_data\n",
    "        return frames[0], frames[1], frames[2], frames[3], frames[4], frames[5], self.labels[idx]\n",
    "vin = (b_size,)+input_shape\n",
    "#summary(cnn, [vin],\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\"])\n",
    "\n",
    "#summary(model, [vin, vin, vin, vin, vin, vin],\n",
    "#        col_names=[\"input_size\", \"output_size\", \"num_params\"])\n",
    "#print(model)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593,
     "referenced_widgets": [
      "d37e4c2ed0af470a9906ed27bd3648da",
      "6cbf3764755248be8af176cc5b8a8dbf",
      "76154046d4f84a238c9f2f2a37656e57",
      "8143aba1b82e49e4b76948bdb805f3bf",
      "5d3b66f6942d471cb24b35c01c5d3d28",
      "a6cc1ea9a5b5445f905effa30c8892aa",
      "6ad7e8687bb44cdeb4c287828c027357",
      "c087fe286ec74047b96c9f3d6e6dbf52",
      "58e6a9df6084459cbf7fd0dcd182d718",
      "6965d7602b31427dad74605500aeea29",
      "0244ec4230b045eda16c837286fd633f"
     ]
    },
    "id": "rqZYYTIST6_y",
    "executionInfo": {
     "status": "error",
     "timestamp": 1730148634937,
     "user_tz": -60,
     "elapsed": 12860,
     "user": {
      "displayName": "nassim moualek",
      "userId": "09838228510010691223"
     }
    },
    "outputId": "786a641a-3002-45e2-a9e2-8dd2530cdee6",
    "ExecuteTime": {
     "end_time": "2024-10-29T05:10:52.909640Z",
     "start_time": "2024-10-29T05:09:59.212887Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Started at =\", current_time)\n",
    "datasets = [ 'trainm', 'SCVD', 'RWF-2000', 'rvdata' ]\n",
    "dpages = [ 30, 224, 200, 200 ]\n",
    "dsizes = [ 150, 224, 800, 1000 ]\n",
    "nbr_it = 3\n",
    "epochs = 1\n",
    "LOADED=False\n",
    "for it in range(nbr_it):\n",
    "  print('#### IT '+str(it))\n",
    "  for dt_index in range( len(datasets)):\n",
    "    dataset = datasets[dt_index]\n",
    "    page_size = dpages[dt_index]\n",
    "    size = dsizes[dt_index]\n",
    "    pages = int(size/page_size)\n",
    "    for k in range(pages):\n",
    "      fts_train = np.zeros( (2*page_size,) + full_input_shape, dtype=dtype )\n",
    "      labs_train = np.empty(2*page_size, dtype=dtype)\n",
    "      use_previous_load = (it==0 and dt_index==0 and k==0 and LOADED )\n",
    "      if use_previous_load==False:\n",
    "        loadData( dataset, k*page_size, (k+1)*page_size, fts_train, labs_train)\n",
    "      else:\n",
    "        print(\"Using previous load\")\n",
    "        LOADED = (it==0 and dt_index==0 and k==0)\n",
    "      random_state = np.random.randint(1, 100)\n",
    "      X_train, fights_test, Y_train, labels_test = train_test_split(fts_train,\n",
    "                          labs_train, test_size=0.2, random_state=random_state)\n",
    "      y_train = tf_utils.to_categorical(Y_train, num_classes=num_classes)\n",
    "      y_test = tf_utils.to_categorical(labels_test, num_classes=num_classes)\n",
    "      from datetime import datetime\n",
    "      import time\n",
    "      millis = int(round(time.time() * 1000))\n",
    "      classification_dataset_train = ClassificationDataset(dataset=X_train, labels=y_train)\n",
    "      classification_dataset_test = ClassificationDataset(dataset=fights_test, labels=y_test)\n",
    "      classification_dataloader_train = DataLoader(dataset=classification_dataset_train,\n",
    "                                                 batch_size=b_size,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_workers=0,\n",
    "                                                 drop_last=False)\n",
    "      classification_dataloader_test = DataLoader(dataset=classification_dataset_test,\n",
    "                                             batch_size=b_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=0,\n",
    "                                             drop_last=False)\n",
    "     # torch.autograd.set_detect_anomaly(True)\n",
    "      results = multiInputTrain(model=model,\n",
    "                       train_dataloader=classification_dataloader_train,\n",
    "                       test_dataloader=classification_dataloader_test,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=epochs,\n",
    "                       device=device)\n",
    "      if RunningInCOLAB==False:\n",
    "        break\n",
    "    if RunningInCOLAB==False:\n",
    "        break  \n",
    "  v1, v2, v3, v4, v5, v6, _ = classification_dataset_test.getItem(0)\n",
    "  v1 = v1.view(-1, 3, 300, 300).to(device)\n",
    "  v2 = v2.view(-1, 3, 300, 300).to(device)\n",
    "  v3 = v3.view(-1, 3, 300, 300).to(device)\n",
    "  v4 = v4.view(-1, 3, 300, 300).to(device)\n",
    "  v5 = v5.view(-1, 3, 300, 300).to(device)\n",
    "  v6 = v6.view(-1, 3, 300, 300).to(device)\n",
    "  print('Saving ...')\n",
    "  if RunningInCOLAB:    \n",
    "      pt = \"drive/My Drive/fights\"+str(it)+\".pt\"\n",
    "      torch.save(model, pt)\n",
    "      torch.load(pt)\n",
    "      onnx = \"drive/My Drive/fights\"+str(it)+\".onnx\"\n",
    "      torch_input = torch.randn( (1,) + full_input_shape )\n",
    "      onnx_program = torch.onnx.export(model, torch_input, onnx)\n",
    "      #onnx_program.save(onnx)\n",
    "  else:\n",
    "      dataFolder = '/home/mac/data/'\n",
    "      pt = dataFolder + \"fights.pt\"\n",
    "      torch.jit.save(model, pt)\n",
    "      #torch.load(pt)\n",
    "      torch.jit.load(pt)\n",
    "      onnx = dataFolder + \"fights_pt.onnx\"\n",
    "      torch_input = torch.randn( (1,) + full_input_shape )\n",
    "      #onnx_program = torch.onnx.export(model, (v1, v2, v3, v4, v5, v6), onnx, opset_version=16)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at = 06:09:59\n",
      "#### IT 0\n",
      "Loading trainm from 0 to 30\n",
      "Readed 30\n",
      "Readed 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 48\u001B[0m\n\u001B[1;32m     42\u001B[0m  classification_dataloader_test \u001B[38;5;241m=\u001B[39m DataLoader(dataset\u001B[38;5;241m=\u001B[39mclassification_dataset_test,\n\u001B[1;32m     43\u001B[0m                                         batch_size\u001B[38;5;241m=\u001B[39mb_size,\n\u001B[1;32m     44\u001B[0m                                         shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     45\u001B[0m                                         num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     46\u001B[0m                                         drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# torch.autograd.set_detect_anomaly(True)\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m  results \u001B[38;5;241m=\u001B[39m \u001B[43mmultiInputTrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclassification_dataloader_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclassification_dataloader_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m                  \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m  \u001B[38;5;28;01mif\u001B[39;00m RunningInCOLAB\u001B[38;5;241m==\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m     56\u001B[0m    \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[13], line 142\u001B[0m, in \u001B[0;36mmultiInputTrain\u001B[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(epochs)):\n\u001B[0;32m--> 142\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     test_loss, test_acc \u001B[38;5;241m=\u001B[39m test_step(model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    148\u001B[0m       dataloader\u001B[38;5;241m=\u001B[39mtest_dataloader,\n\u001B[1;32m    149\u001B[0m       loss_fn\u001B[38;5;241m=\u001B[39mloss_fn,\n\u001B[1;32m    150\u001B[0m       device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;66;03m# Print out what's happening\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[13], line 28\u001B[0m, in \u001B[0;36mtrain_step\u001B[0;34m(model, dataloader, loss_fn, optimizer, device)\u001B[0m\n\u001B[1;32m     25\u001B[0m X1, X2, X3, X4, X5, X6, y \u001B[38;5;241m=\u001B[39m X1\u001B[38;5;241m.\u001B[39mto(device), X2\u001B[38;5;241m.\u001B[39mto(device),X3\u001B[38;5;241m.\u001B[39mto(device),X4\u001B[38;5;241m.\u001B[39mto(device),X5\u001B[38;5;241m.\u001B[39mto(device),X6\u001B[38;5;241m.\u001B[39mto(device),y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# 1. Forward pass\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX4\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX5\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX6\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m debugMode:\n\u001B[1;32m     30\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[14], line 30\u001B[0m, in \u001B[0;36mMultiInputs.forward\u001B[0;34m(self, input1, input2, input3, input4, input5, input6)\u001B[0m\n\u001B[1;32m     28\u001B[0m c3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcnn(input3)\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1536\u001B[39m)\n\u001B[1;32m     29\u001B[0m c4 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcnn(input4)\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1536\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m c5 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput5\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1536\u001B[39m)\n\u001B[1;32m     31\u001B[0m c6 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcnn(input6)\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1536\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mall\u001B[39m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((c1, c2, c3, c4, c5, c6), \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torchvision/models/efficientnet.py:343\u001B[0m, in \u001B[0;36mEfficientNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torchvision/models/efficientnet.py:333\u001B[0m, in \u001B[0;36mEfficientNet._forward_impl\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 333\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavgpool(x)\n\u001B[1;32m    336\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torchvision/models/efficientnet.py:164\u001B[0m, in \u001B[0;36mMBConv.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 164\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_res_connect:\n\u001B[1;32m    166\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstochastic_depth(result)\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/activation.py:396\u001B[0m, in \u001B[0;36mSiLU.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 396\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msilu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/functional.py:2058\u001B[0m, in \u001B[0;36msilu\u001B[0;34m(input, inplace)\u001B[0m\n\u001B[1;32m   2056\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(silu, (\u001B[38;5;28minput\u001B[39m,), \u001B[38;5;28minput\u001B[39m, inplace\u001B[38;5;241m=\u001B[39minplace)\n\u001B[1;32m   2057\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[0;32m-> 2058\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msilu_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2059\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39msilu(\u001B[38;5;28minput\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T05:16:48.191021Z",
     "start_time": "2024-10-29T05:15:58.664965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cpu_model = model.cpu()\n",
    "traced_cpu = torch.jit.trace(cpu_model, (v1, v2, v3, v4, v5, v6))\n",
    "dataFolder = '/home/mac/data/'\n",
    "pt = dataFolder + \"fights.pt\"\n",
    "torch.jit.save(traced_cpu, pt)\n",
    "#torch.load(pt)\n",
    "torch.jit.load(pt)\n",
    "onnx = dataFolder + \"fights_pt.onnx\"\n",
    "torch_input = torch.randn( (1,) + full_input_shape )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/home/mac/miniconda3/envs/ai/lib/python3.10/site-packages/torch/jit/_trace.py:1084: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 2 / 2 (100.0%)\n",
      "Greatest absolute difference: 0.25275176763534546 at index (0, 0) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 1.7463545294848888 at index (0, 0) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "d37e4c2ed0af470a9906ed27bd3648da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6cbf3764755248be8af176cc5b8a8dbf",
       "IPY_MODEL_76154046d4f84a238c9f2f2a37656e57",
       "IPY_MODEL_8143aba1b82e49e4b76948bdb805f3bf"
      ],
      "layout": "IPY_MODEL_5d3b66f6942d471cb24b35c01c5d3d28"
     }
    },
    "6cbf3764755248be8af176cc5b8a8dbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6cc1ea9a5b5445f905effa30c8892aa",
      "placeholder": "",
      "style": "IPY_MODEL_6ad7e8687bb44cdeb4c287828c027357",
      "value": "100%"
     }
    },
    "76154046d4f84a238c9f2f2a37656e57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c087fe286ec74047b96c9f3d6e6dbf52",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_58e6a9df6084459cbf7fd0dcd182d718",
      "value": 1
     }
    },
    "8143aba1b82e49e4b76948bdb805f3bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6965d7602b31427dad74605500aeea29",
      "placeholder": "",
      "style": "IPY_MODEL_0244ec4230b045eda16c837286fd633f",
      "value": "1/1[00:03&lt;00:00,3.90s/it]"
     }
    },
    "5d3b66f6942d471cb24b35c01c5d3d28": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6cc1ea9a5b5445f905effa30c8892aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ad7e8687bb44cdeb4c287828c027357": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c087fe286ec74047b96c9f3d6e6dbf52": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58e6a9df6084459cbf7fd0dcd182d718": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6965d7602b31427dad74605500aeea29": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0244ec4230b045eda16c837286fd633f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
